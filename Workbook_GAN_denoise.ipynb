{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![im](./assests/banner.jpeg)\n",
        "\n",
        "#### Associated Group Works\n",
        "> Alis, Deniz, et al. \"Deep Learning for Assessing Image Quality in Bi-Parametric Prostate MRI: A Feasibility Study.\" European Journal of Radiology (2023): 110924. https://doi.org/10.1016/j.ejrad.2023.110924<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4OzNX46_iZc"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/MSaidKartal/denoise-pix2pix.git' && cd ../\n",
        "!pip install -r denoise-pix2pix/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0LGj-I8TRm9"
      },
      "source": [
        "### Preamble to the Hands-On Workshop on GANs for Prostate MRI Denoising\n",
        "\n",
        "#### Introduction\n",
        "Welcome to our hands-on workshop dedicated to exploring the innovative application of Generative Adversarial Networks (GANs) in the field of medical imaging, particularly focusing on denoising prostate MRI scans. This workshop is designed for radiologists and researchers interested in the intersection of artificial intelligence and medical imaging. Our goal is to provide a comprehensive understanding of how GANs can be leveraged to enhance the clarity and utility of prostate MRI scans, a crucial tool in the diagnosis and management of prostate cancer.\n",
        "\n",
        "#### Workshop Overview\n",
        "Throughout this workshop, participants will engage in a series of practical exercises and discussions centered around the implementation and optimization of GANs for medical imaging tasks. We will delve into the fundamentals of GANs, their architecture, and the nuances of their training process, tailored specifically for the challenges presented in prostate MRI scans.\n",
        "\n",
        "#### Objectives\n",
        "1. **Understanding GAN Architecture**: Gain insights into the components of GANs – the discriminator and generator – and how they interact in the adversarial training process.\n",
        "2. **Practical Application**: Apply GANs to real-world prostate MRI data, learning how to preprocess data, train models, and evaluate their performance using key metrics like PSNR, SSIM, and MAE.\n",
        "3. **Denoising Techniques**: Explore advanced denoising techniques using GANs to improve the quality of MRI scans, enhancing their diagnostic value.\n",
        "4. **Critical Analysis**: Critically analyze the outcomes, discussing the strengths, limitations, and potential improvements in applying GANs to medical imaging.\n",
        "\n",
        "#### Data and Tools\n",
        "Participants will work with a subset of data from the PI-CAI (Prostate Imaging: Cancer AI) Grand Challenge, offering a rich dataset for hands-on experience. The workshop will primarily utilize Python and deep learning frameworks to implement and test GAN models.\n",
        "\n",
        "#### Target Audience\n",
        "This workshop is tailored for radiologists, AI researchers, and anyone with an interest in the application of deep learning to medical imaging. A basic understanding of deep learning concepts and familiarity with Python programming is recommended to fully benefit from this workshop.\n",
        "\n",
        "#### Conclusion\n",
        "By the end of this workshop, participants will have a clear understanding of how GANs can be used to significantly improve the quality of prostate MRI scans. We aim to empower you with both theoretical knowledge and practical skills, paving the way for future innovations in medical imaging.\n",
        "\n",
        "We look forward to your active participation and the insightful discussions that will emerge, driving forward the field of radiology with AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjhHY_EbUidR"
      },
      "outputs": [],
      "source": [
        "import os  # For interacting with the operating system\n",
        "import sys  # For system-specific parameters and functions\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np  # For numerical operations\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "from keras.models import load_model  # For loading trained Keras models\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and test sets\n",
        "import matplotlib.pyplot as plt  # For plotting graphs\n",
        "import seaborn as sns  # For making attractive and informative statistical graphics\n",
        "\n",
        "sys.path.append(\"denoise-pix2pix/\")  # Add the 'denoise-pix2pix' directory to the system path for module import\n",
        "\n",
        "# Importing custom functions and classes from the denoise-pix2pix project\n",
        "from pix2pix import define_discriminator, define_generator, define_gan, train\n",
        "from datasets import load_case, DataLoader\n",
        "from plot_utils import interactive_show, interactive_inference, mae_calc\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr  # For PSNR calculation\n",
        "from skimage.metrics import structural_similarity as compare_ssim  # For SSIM calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwAppSg5PDEG"
      },
      "outputs": [],
      "source": [
        "# Check if the 'data' directory exists, if not, download and unzip the dataset\n",
        "if not os.path.exists('data'):  # Condition to check existence of 'data' directory\n",
        "    !wget -q https://huggingface.co/datasets/msaidkartal/denoise-prostateMRI/resolve/main/data.zip  # Download dataset zip file quietly\n",
        "    !unzip -q data.zip  # Unzip the downloaded dataset quietly\n",
        "\n",
        "# Check if the 'models' directory exists, if not, download and unzip the pre-trained models\n",
        "if not os.path.exists('models'):  # Condition to check existence of 'models' directory\n",
        "    !wget -q https://huggingface.co/datasets/msaidkartal/denoise-prostateMRI/resolve/main/models.zip  # Download model zip file quietly\n",
        "    !unzip -q models.zip  # Unzip the downloaded model quietly\n",
        "\n",
        "# Note: The -q flag in wget and unzip commands ensures quiet (non-verbose) operation with no unnecessary output to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymaLRgt26o_L"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary 'dirs' to store paths for various components used in the project\n",
        "dirs = {\n",
        "    'low_res': 'data/low_res',  # Path to the directory containing low resolution (noisy) MRI images\n",
        "    'high_res': 'data/source',  # Path to the directory containing high resolution (original) MRI images\n",
        "    'metrics': 'data/metrics.xlsx',  # Path to an Excel file containing metrics data\n",
        "    'model': 'models/best_model.h5'  # Path to the pre-trained model file\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J75nqzH5TRm_"
      },
      "source": [
        "### Dataset Overview: PI-CAI Grand Challenge Subset\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/picai.png?raw=1)\n",
        "\n",
        "In this workshop, we are utilizing a subset of the data from the Prostate Imaging: Cancer AI (PI-CAI) Grand Challenge. This pioneering initiative is a comprehensive effort to harness the power of artificial intelligence in the diagnosis and detection of prostate cancer through advanced imaging techniques.\n",
        "\n",
        "#### Key Features of the Dataset:\n",
        "1. **Origin:** Our dataset is derived from the extensive collection of over 10,000 carefully curated prostate MRI exams made available through the PI-CAI Grand Challenge. This rich dataset is pivotal for validating modern AI algorithms in the field of radiology, especially in the context of clinically significant prostate cancer (csPCa).\n",
        "\n",
        "2. **Composition:** The subset we are working with includes high-quality bi-parametric MRI (bpMRI) scans. These scans are instrumental in identifying csPCa, offering a comprehensive view of the prostate region with varying degrees of resolution and clarity.\n",
        "\n",
        "3. **Purpose and Utility:** The primary objective of using this dataset is to demonstrate and validate the effectiveness of AI algorithms, particularly in the patient-level diagnosis and lesion-level detection of csPCa. Our focus will be on exploring how deep learning models can be trained and tested to enhance diagnostic accuracy in prostate cancer detection.\n",
        "\n",
        "4. **Challenges and Opportunities:** Working with this dataset presents an extraordinary opportunity to delve into the nuances of medical imaging in AI. It allows us to tackle challenges such as varying image quality, the subtlety of lesion appearances, and the complexities of cancer detection in MRI scans.\n",
        "\n",
        "5. **Educational Value:** For participants of this workshop, engaging with this dataset offers a hands-on experience in applying deep learning techniques to real-world medical imaging problems. It is an excellent opportunity to understand the intricacies of AI applications in healthcare, specifically in the domain of cancer diagnosis.\n",
        "\n",
        "https://pi-cai.grand-challenge.org/PI-CAI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7X7fjG-PC_j"
      },
      "outputs": [],
      "source": [
        "# Load and process the metrics data\n",
        "data = pd.read_excel(dirs['metrics'], index_col=0)  # Load the metrics data from Excel file into a pandas DataFrame. 'index_col=0' sets the first column as the index.\n",
        "\n",
        "data = data.reset_index(drop=True)  # Reset the index of the DataFrame, dropping the old index.\n",
        "\n",
        "data.head(10)  # Display the first 10 rows of the DataFrame for a quick overview."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zkbVdpFpTRnA"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Loading and visualizing a specific case from the dataset\n",
        "index = 15  # Set the index for the patient case to be loaded\n",
        "\n",
        "vol = load_case(data.PatientName[index])  # Load the MRI volume for the specified patient case using the 'load_case' function\n",
        "\n",
        "interactive_show(vol)  # Display the loaded MRI volume using an interactive viewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GxGT8wMpIZQ"
      },
      "outputs": [],
      "source": [
        "# Loading and visualizing a specific case from the dataset\n",
        "index = 15  # Set the index for the patient case to be loaded\n",
        "\n",
        "vol = load_case(data.PatientName[index])  # Load the MRI volume for the specified patient case using the 'load_case' function\n",
        "\n",
        "interactive_show(\"fill here\") # Display the loaded MRI volume using an interactive viewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdUu5xTnumJd"
      },
      "outputs": [],
      "source": [
        "model = load_model(dirs['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3WUa45ICMzb"
      },
      "outputs": [],
      "source": [
        "# Loading preprocessed low and high resolution images for a specific case\n",
        "index = 100  # Set the index for the patient case to be loaded\n",
        "\n",
        "# Load both low and high resolution images for the specified patient case\n",
        "# The 'preprocess' flag is set to True, indicating that preprocessing steps will be applied to the images\n",
        "low_res, high_res = load_case(data.PatientName[index], preprocess=True)\n",
        "\n",
        "# Generating denoised images using the pre-trained model\n",
        "# 'model.predict' is used to generate images from the low resolution input, with no output verbosity\n",
        "generated_im = model.predict(low_res, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zHKUPArDO_o"
      },
      "outputs": [],
      "source": [
        "# Calculating and comparing image quality metrics between high-resolution and generated images\n",
        "rangeD = high_res.max() - high_res.min()  # Determine the range of pixel values in the high-resolution images\n",
        "\n",
        "# Compute the Structural Similarity Index (SSIM) for comparing the similarity between high-res and generated images\n",
        "ssim = compare_ssim(high_res[:,:,:,0], generated_im[:,:,:,0], data_range=rangeD)\n",
        "\n",
        "# Compute the Peak Signal-to-Noise Ratio (PSNR) for assessing the quality of the generated images against high-res images\n",
        "psnr = compare_psnr(high_res[:,:,:,0], generated_im[:,:,:,0], data_range=rangeD)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE) to measure the average magnitude of errors between high-res and generated images\n",
        "mae = mae_calc(high_res[:,:,:,0], generated_im[:,:,:,0])\n",
        "\n",
        "# Print out the calculated PSNR, SSIM, and MAE values\n",
        "print(f\"PSNR: {psnr:.3f}\\nSSIM: {ssim:.3f}\\nMAE: {mae:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-oa9-N_TRnB"
      },
      "source": [
        "#### PSNR (Peak Signal-to-Noise Ratio):\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/psnr1.png?raw=1)\n",
        "* PSNR is a widely used metric to measure the quality of reconstructed or generated images compared to the original high-resolution images.\n",
        "* It is expressed in decibels (dB), with higher values indicating better image quality.\n",
        "* PSNR evaluates the ratio between the maximum possible power of a signal (in this case, pixel intensity) and the power of corrupting noise.\n",
        "* In the context of image denoising, a higher PSNR means the denoised image is closer to the original image, indicating better performance of the denoising algorithm.\n",
        "\n",
        "#### SSIM (Structural Similarity Index):\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/psnr2.png?raw=1)\n",
        "* SSIM is used to measure the similarity between two images, in this case, the generated image and the original high-resolution image.\n",
        "* Unlike PSNR that focuses on pixel-level differences, SSIM considers changes in structural information, luminance, and contrast.\n",
        "* SSIM values range from -1 to 1, with 1 indicating perfect similarity. A higher SSIM value suggests that the structural integrity and visual perception of the generated image are more aligned with the original.\n",
        "#### MAE (Mean Absolute Error):\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/psnr3.png?raw=1)\n",
        "* MAE is a straightforward measure of the average magnitude of errors between the paired observations, here between pixels of the generated and original images.\n",
        "* It calculates the absolute difference between corresponding pixels of the two images and then averages these differences over all pixels.\n",
        "* A lower MAE indicates that the generated image is closer to the original, signifying a more accurate reconstruction or denoising process.\n",
        "\n",
        "In summary, these metrics (PSNR, SSIM, MAE) provide a comprehensive assessment of the generated image's quality compared to the original, covering aspects like noise reduction, structural similarity, and overall error magnitude. They are critical in evaluating the effectiveness of deep learning models in medical imaging tasks such as MRI denoising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EMsCnUTpTRnB"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "interactive_inference((low_res, high_res, generated_im))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW4fPIO-Hugh"
      },
      "outputs": [],
      "source": [
        "interactive_inference((\"fill here\", \"fill here\", \"fill here\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO-t3yN8TRnC"
      },
      "source": [
        "### Introduction to Generative AI\n",
        "#### What is Generative AI?\n",
        "Generative AI encompasses a range of artificial intelligence techniques designed to create new content, including images, text, and complex data structures like medical imaging scans. This branch of AI focuses on generating new data that closely resembles authentic samples, going beyond traditional AI models that are typically used for interpretation or classification.\n",
        "#### Core Techniques in Generative AI:\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/gen1.png?raw=1)\n",
        "1. **Generative Adversarial Networks (GANs):** Consist of two parts, a generator and a discriminator, working together to improve the quality of generated outputs.\n",
        "2. **Variational Autoencoders (VAEs):** Aim to compress data into a lower-dimensional representation and then reconstruct it, trying to retain as much original information as possible.\n",
        "3. **Diffusion Models:** These are a newer class of generative models that transform patterns of noise into coherent images or structures through a gradual refining process.\n",
        "4. **Large Language Models (LLMs):** Specialized in generating human-like text, LLMs like GPT (Generative Pre-trained Transformer) are trained on vast amounts of textual data to produce contextually relevant and coherent language outputs.\n",
        "\n",
        "#### Applications in Healthcare:\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/gen2.png?raw=1)\n",
        "Generative AI holds transformative potential in healthcare. It can be used for synthesizing medical images, augmenting datasets for machine learning models, developing personalized medicine strategies, and even in generating medical literature or reports. The accuracy and efficiency of patient care can be significantly enhanced through these applications.\n",
        "#### Relevance to This Workshop:\n",
        "Our focus will be on using GANs for denoising prostate MRI scans, a prime example of generative AI's capability to improve medical imaging quality. We'll explore how these advanced AI techniques can refine noisy images, thereby enhancing their diagnostic value in a clinical setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj0lik3CIMMo"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and testing subsets\n",
        "train_case, test_case, train_y, test_y = train_test_split(\n",
        "    data['PatientName'].tolist(),  # List of patient names to be used as features for splitting\n",
        "    data['SSIM'].tolist(),  # List of SSIM values to be used as labels for splitting\n",
        "    test_size=0.20,  # 20% of the data is allocated for the test set\n",
        "    random_state=0  # Set a random state for reproducibility of the split\n",
        ")\n",
        "\n",
        "# Display the number of cases in the training and testing sets\n",
        "len(train_case), len(test_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1hwGg8Vynsms"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def model_test(test_case, model=model, input_shape=(512,512)):\n",
        "\n",
        "  ssims = []\n",
        "  psnrs = []\n",
        "  maes = []\n",
        "\n",
        "  cases = []\n",
        "\n",
        "  genssims = []\n",
        "  genpsnrs = []\n",
        "  genmaes = []\n",
        "  for patient_num in tqdm(test_case):\n",
        "\n",
        "    low_res, high_res = load_case(patient_num, preprocess=True, input_shape=input_shape)\n",
        "    generated_im = model.predict(low_res, verbose=0)\n",
        "\n",
        "    rangeD = high_res.max() - high_res.min()\n",
        "\n",
        "    genssim = compare_ssim(high_res[:,:,:,0], generated_im[:,:,:,0], data_range=rangeD)\n",
        "    genpsnr = compare_psnr(high_res[:,:,:,0], generated_im[:,:,:,0], data_range=rangeD)\n",
        "    genmae = mae_calc(high_res[:,:,:,0], generated_im[:,:,:,0])\n",
        "\n",
        "    genssims.append(genssim)\n",
        "    genpsnrs.append(genpsnr)\n",
        "    genmaes.append(genmae)\n",
        "    cases.append(patient_num)\n",
        "\n",
        "    rangeD = high_res.max() - high_res.min()\n",
        "\n",
        "    ssim = compare_ssim(high_res[:,:,:,0], low_res[:,:,:,0], data_range=rangeD)\n",
        "    psnr = compare_psnr(high_res[:,:,:,0], low_res[:,:,:,0], data_range=rangeD)\n",
        "    mae = mae_calc(high_res[:,:,:,0], low_res[:,:,:,0])\n",
        "\n",
        "    ssims.append(ssim)\n",
        "    psnrs.append(psnr)\n",
        "    maes.append(mae)\n",
        "\n",
        "  return pd.DataFrame({'PatientName':cases, 'PSNR':psnrs, 'SSIM':ssims, 'MAE':maes,\n",
        "                       'GenPSNR':genpsnrs, 'GenSSIM':genssims, 'GenMAE':genmaes})\n",
        "\n",
        "\n",
        "test_df = model_test(test_case)\n",
        "test_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFWg-UVVTRnC"
      },
      "outputs": [],
      "source": [
        "def model_test(test_case, model=model, input_shape=(512,512)):\n",
        "\n",
        "  ssims = []\n",
        "  psnrs = []\n",
        "  maes = []\n",
        "\n",
        "  cases = []\n",
        "\n",
        "  genssims = []\n",
        "  genpsnrs = []\n",
        "  genmaes = []\n",
        "  for patient_num in tqdm(test_case):\n",
        "\n",
        "    low_res, high_res = load_case(patient_num, preprocess=True, input_shape=input_shape)\n",
        "    generated_im = model.predict(low_res, verbose=0)\n",
        "\n",
        "    rangeD = high_res.max() - high_res.min()\n",
        "\n",
        "    genssim = compare_ssim(high_res[:,:,:,0], generated_im[:,:,:,0], data_range=rangeD)\n",
        "    genpsnr = compare_psnr(high_res[:,:,:,0], generated_im[:,:,:,0], data_range=rangeD)\n",
        "    genmae = mae_calc(high_res[:,:,:,0], generated_im[:,:,:,0])\n",
        "\n",
        "    genssims.append(genssim)\n",
        "    genpsnrs.append(genpsnr)\n",
        "    genmaes.append(genmae)\n",
        "    cases.append(patient_num)\n",
        "\n",
        "    rangeD = high_res.max() - high_res.min()\n",
        "\n",
        "    ssim = compare_ssim(high_res[:,:,:,0], low_res[:,:,:,0], data_range=rangeD)\n",
        "    psnr = compare_psnr(high_res[:,:,:,0], low_res[:,:,:,0], data_range=rangeD)\n",
        "    mae = mae_calc(high_res[:,:,:,0], low_res[:,:,:,0])\n",
        "\n",
        "    ssims.append(ssim)\n",
        "    psnrs.append(psnr)\n",
        "    maes.append(mae)\n",
        "\n",
        "  return pd.DataFrame({'PatientName':\"fill here\", 'PSNR':\"fill here\", 'SSIM':\"fill here\", 'MAE':\"fill here\",\n",
        "                       'GenPSNR':\"fill here\", 'GenSSIM':\"fill here\", 'GenMAE':\"fill here\"})\n",
        "\n",
        "test_df = model_test(test_case)\n",
        "test_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQk84ZLzpZ4n"
      },
      "outputs": [],
      "source": [
        "# Creating a 2x2 subplot layout\n",
        "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
        "\n",
        "# Plotting the first boxplot for PSNR\n",
        "sns.boxplot(data=test_df[['PSNR', 'GenPSNR']], orient='h', ax=axs[0, 0])\n",
        "axs[0, 0].set_title('Low Res(Input MRI) vs Generated MRI PSNR')\n",
        "\n",
        "# Plotting the second boxplot for SSIM\n",
        "sns.boxplot(data=test_df[['SSIM', 'GenSSIM']], orient='h', ax=axs[0, 1])\n",
        "axs[0, 1].set_title('Low Res(Input MRI) vs Generated MRI SSIM')\n",
        "\n",
        "# Plotting the third boxplot for MAE\n",
        "sns.boxplot(data=test_df[['MAE', 'GenMAE']], orient='h', ax=axs[1, 0])\n",
        "axs[1, 0].set_title('Low Res(Input MRI) vs Generated MRI MAE')\n",
        "\n",
        "# Leaving the bottom-right subplot empty\n",
        "axs[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQY3i3vXKqXw"
      },
      "outputs": [],
      "source": [
        "# Creating a DataLoader instance for the training dataset\n",
        "train_dataset = DataLoader(\n",
        "    train_case,  # List of patient cases to be included in the training dataset\n",
        "    shape=(256, 256)  # Specifying the shape (resolution) for the images in the dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXiwCsEWTRnC"
      },
      "source": [
        "### Introduction to Generative Adversarial Networks (GANs)\n",
        "Generative Adversarial Networks (GANs) are a fascinating and powerful class of artificial intelligence models used in the field of machine learning, particularly for the task of generative modeling. They were introduced by Ian Goodfellow and his colleagues in 2014 and have since revolutionized the way we think about generating new content, whether it's images, text, or even complex structures like 3D models.\n",
        "#### Fundamental Concepts of GANs\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/gan1.png?raw=1)\n",
        "A GAN consists of two main components that work in tandem:\n",
        "\n",
        "1. **Generator:** This part of the GAN is responsible for creating new data. It takes in random noise as an input and transforms it into a data output (e.g., an image). The generator's goal is to produce data that is indistinguishable from real data.\n",
        "\n",
        "2. **Discriminator:** The discriminator acts as a critic that tries to differentiate between real data (from the training set) and fake data (created by the generator). It is essentially a binary classifier that learns to identify whether a given input is real or generated.\n",
        "\n",
        "#### The Training Process\n",
        "The training of a GAN involves a competitive game between the generator and the discriminator:\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/gan2.png?raw=1)\n",
        "* The **generator** is trained to produce increasingly realistic data, trying to fool the discriminator.\n",
        "\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/disc.jpg?raw=1)\n",
        "* The **discriminator** is trained to get better at distinguishing real data from the fakes created by the generator.\n",
        "\n",
        "This process is akin to a forger trying to create a perfect fake painting, and an art expert trying to detect the forgery. Over time, the forger becomes skilled at creating realistic art, while the expert becomes better at spotting fakes.\n",
        "\n",
        "#### Applications\n",
        "GANs have a wide range of applications, including but not limited to:\n",
        "\n",
        "* **Image Generation:** Creating realistic images from scratch.\n",
        "* **Data Augmentation:** Generating new data for training machine learning models.\n",
        "* **Style Transfer:** Modifying images to change their style (e.g., changing a day scene to night).\n",
        "* **Medical Imaging:** Enhancing the quality of medical images or creating synthetic medical data for research and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yw0yykz9K-oN"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Setting up the GAN components: discriminator, generator, and the composite model\n",
        "image_shape = (256, 256, 1)  # Define the shape of the images (256x256 pixels with 1 color channel)\n",
        "\n",
        "# Initialize the discriminator model\n",
        "discriminator_model = define_discriminator(image_shape)  # Create the discriminator model with the specified image shape\n",
        "\n",
        "# Initialize the generator model\n",
        "generator_model = define_generator(image_shape)  # Create the generator model with the same image shape\n",
        "\n",
        "# Define the composite GAN model\n",
        "gan_model = define_gan(generator_model, discriminator_model, image_shape) # Combine generator and discriminator into the GAN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyIGtdswTRnD"
      },
      "outputs": [],
      "source": [
        "# Setting up the GAN components: discriminator, generator, and the composite model\n",
        "image_shape = (256, 256, 1)  # Define the shape of the images (256x256 pixels with 1 color channel)\n",
        "\n",
        "# Initialize the discriminator model\n",
        "discriminator_model = define_discriminator(image_shape)  # Create the discriminator model with the specified image shape\n",
        "\n",
        "# Initialize the generator model\n",
        "generator_model = define_generator(image_shape)  # Create the generator model with the same image shape\n",
        "\n",
        "# Define the composite GAN model\n",
        "gan_model = define_gan(\"fill here\", \"fill here\", \"fill here\")  # Combine generator and discriminator into the GAN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rxc6_f3TRnD"
      },
      "source": [
        "### GAN Training Process in Medical Imaging\n",
        "In the context of our workshop, where we use GANs for denoising prostate MRI scans, understanding the training process is crucial. Here's an overview of how GANs are trained, particularly focusing on the dynamics between the generator and discriminator components.\n",
        "\n",
        "#### The Training Dynamics:\n",
        "1. **Initial Phase:** At the start of training, the generator produces images that are far from the desired quality, and the discriminator easily differentiates between real and generated images.\n",
        "\n",
        "2. **Iterative Improvement:** Both the generator and discriminator improve through iterations. The generator learns to produce more realistic images, while the discriminator becomes more adept at distinguishing fakes from real images.\n",
        "\n",
        "3. **Feedback Loop:** The generator is guided by the feedback it receives from the discriminator. If the discriminator easily identifies a generated image, the generator adjusts to produce more convincing images.\n",
        "\n",
        "Key Training Steps:\n",
        "![im](https://github.com/MSaidKartal/denoise-pix2pix/blob/main/assests/train1.png?raw=1)\n",
        "1. **Training the Discriminator:** In each training iteration, the discriminator is trained first. It is fed with a batch of real images and a batch of images generated by the generator. The goal is for the discriminator to learn to label real images as real and generated images as fake.\n",
        "\n",
        "2. **Training the Generator:** Next, the generator is trained. The objective is to create images that the discriminator classifies as real. The generator's success is measured by how often the discriminator mistakes its output for real images.\n",
        "\n",
        "3. **Loss Functions:** The loss functions play a critical role. For the discriminator, the loss is high when it incorrectly classifies images. For the generator, the loss is high when the discriminator correctly identifies its images as fake.\n",
        "\n",
        "4. **Backpropagation and Updates:** Both the generator and discriminator use the backpropagation algorithm to update their weights. The goal is to minimize their respective loss functions.\n",
        "\n",
        "5. **Reaching Equilibrium:** The training continues until the generator produces images indistinguishable from real images to the discriminator. At this point, the discriminator has a 50% success rate, essentially guessing at random.\n",
        "\n",
        "#### Considerations for Medical Imaging:\n",
        "* **Quality and Clarity:** In medical imaging, the quality of generated images is paramount. The training focuses on achieving high clarity and detail, essential for accurate diagnosis.\n",
        "* **Data Sensitivity:** The training must be conducted with an understanding of the sensitivity and specificity required in medical imaging, ensuring that the denoised images maintain all critical information.\n",
        "\n",
        "In our workshop, we will explore the nuances of this training process with a hands-on approach, providing insights into the practical application of GANs in improving the quality of medical images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8RxtoqVXLd1l"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Initiating the training process for the GAN\n",
        "train(\n",
        "    discriminator_model,    # The discriminator model to be trained\n",
        "    generator_model,        # The generator model to be trained\n",
        "    gan_model,              # The composite GAN model\n",
        "    train_dataset,          # The training dataset to be used\n",
        "    n_epochs=1,             # Number of epochs for training (set to 1 for demonstration)\n",
        "    n_batch=1               # Batch size for training (set to 1 for demonstration)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2-z7soWTRnD"
      },
      "outputs": [],
      "source": [
        "# Initiating the training process for the GAN\n",
        "train(\n",
        "    discriminator_model,  # The discriminator model to be trained\n",
        "    \"fill here\",      # The generator model to be trained\n",
        "    \"fill here\",            # The composite GAN model\n",
        "    \"fill here\",        # The training dataset to be used\n",
        "    n_epochs=\"fill here\",           # Number of epochs for training (set to 1 for demonstration)\n",
        "    n_batch=\"fill here\"             # Batch size for training (set to 1 for demonstration)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZleIxdoqtcdP"
      },
      "outputs": [],
      "source": [
        "newtest_df = model_test(test_case, model=generator_model, input_shape=(256,256))\n",
        "newtest_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd4dvmxcuKtj"
      },
      "outputs": [],
      "source": [
        "# Creating a 2x2 subplot layout\n",
        "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
        "\n",
        "# Plotting the first boxplot for PSNR\n",
        "sns.boxplot(data=newtest_df[['PSNR', 'GenPSNR']], orient='h', ax=axs[0, 0])\n",
        "axs[0, 0].set_title('Low Res(Input MRI) vs Generated MRI PSNR')\n",
        "\n",
        "# Plotting the second boxplot for SSIM\n",
        "sns.boxplot(data=newtest_df[['SSIM', 'GenSSIM']], orient='h', ax=axs[0, 1])\n",
        "axs[0, 1].set_title('Low Res(Input MRI) vs Generated MRI SSIM')\n",
        "\n",
        "# Plotting the third boxplot for MAE\n",
        "sns.boxplot(data=newtest_df[['MAE', 'GenMAE']], orient='h', ax=axs[1, 0])\n",
        "axs[1, 0].set_title('Low Res(Input MRI) vs Generated MRI MAE')\n",
        "\n",
        "# Leaving the bottom-right subplot empty\n",
        "axs[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
